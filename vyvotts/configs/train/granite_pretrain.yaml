# Model
model_name: "ibm-granite/granite-4.0-h-1b-base"
tokenizer_name: "ibm-granite/granite-4.0-h-1b-base"

# Training Args
epochs: 1
batch_size: 1  # Reduced to 1 due to Mamba memory requirements
number_processes: 4
pad_token: 100256  # Granite's PAD token
save_steps: 12000
learning_rate: 5.0e-5
ratio: "2:1"
max_seq_length: 4096  # Reduced from 8192 - Mamba state grows quadratically!

# Datasets
text_QA_dataset: "mrfakename/MDF-EN-Emilia-YODAS"
TTS_dataset: "mrfakename/MDF-EN-Emilia-YODAS"

# Naming and paths
save_folder: "checkpoints"
project_name: "granite-tts-transformers"
run_name: "granite-tts-transformers"
